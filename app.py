import re
import csv
import requests
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import altair as alt
from io import StringIO

# Configuraci칩n de Streamlit para hacer el contenido de la derecha ancho completo
st.set_page_config(layout="wide")

# Funci칩n para procesar las URLs
def process_urls(urls):
    regex_published = r'"datePublished":"([^"]+)"'
    regex_modified = r'"dateModified":"([^"]+)"'
    
    results = []
    
    for url in urls:
        try:
            # Obtener el contenido HTML de la p치gina
            response = requests.get(url)
            response.raise_for_status()  # Para lanzar un error si la respuesta no es exitosa
            html = response.text

            # Buscar las fechas usando regex
            published_dates = re.findall(regex_published, html)
            modified_dates = re.findall(regex_modified, html)

            # Considerando solo la primera coincidencia si hay varias
            published_date = published_dates[0] if published_dates else "No encontrado"
            modified_date = modified_dates[0] if modified_dates else "No encontrado"

            # A침adir los resultados a la lista
            results.append({"URL": url, "Fecha de Publicaci칩n": published_date, "Fecha de Modificaci칩n": modified_date})
        
        except Exception as e:
            # En caso de error, a침adir un mensaje de error en los resultados
            results.append({"URL": url, "Fecha de Publicaci칩n": "Error", "Fecha de Modificaci칩n": "Error"})
            st.write(f"Error procesando la URL {url}: {e}")
    
    return results

# Configuraci칩n del Sidebar
st.sidebar.title("Descripci칩n")
st.sidebar.write("""
Esta herramienta permite extraer las fechas de **publicaci칩n** y **modificaci칩n** de URLs que contienen datos estructurados en formato JSON-LD. 

### Stack:
- **Python**
- **Streamlit**
- **Requests**
- **Pandas**
- **Regex**
""")

# Interfaz de usuario de Streamlit
st.title("游뱄 Extractor de 'DatePublished' y 'DateModified' JSON-LD")

# Campo de texto para ingresar las URLs
url_input = st.text_area("Ingresa las URLs (una por l칤nea)")

# Bot칩n para lanzar el proceso
if st.button("Procesar URLs"):
    # Convertir el input en una lista de URLs
    urls = [url.strip() for url in url_input.splitlines() if url.strip()]
    
    if urls:
        # Procesar las URLs
        result_data = process_urls(urls)
        
        # Convertir los resultados en un DataFrame de pandas
        df = pd.DataFrame(result_data)
        
        # Filtrar las filas que no tienen error
        df_filtered = df[(df['Fecha de Publicaci칩n'] != "Error") & (df['Fecha de Publicaci칩n'] != "No encontrado")]

        # Convertir las fechas a formato datetime
        df_filtered['Fecha de Publicaci칩n'] = pd.to_datetime(df_filtered['Fecha de Publicaci칩n'], errors='coerce')
        df_filtered['Fecha de Modificaci칩n'] = pd.to_datetime(df_filtered['Fecha de Modificaci칩n'], errors='coerce')
        
        # Mostrar la tabla en la interfaz
        st.write("Resultados:")
        st.dataframe(df)
        
        # Convertir el DataFrame a CSV para descarga
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)
        csv_data = csv_buffer.getvalue()
        
        # Bot칩n para descargar el CSV
        st.download_button(
            label="Descargar CSV",
            data=csv_data,
            file_name="fechas_publicacion_modificacion.csv",
            mime="text/csv",
        )

        # Visualizaci칩n 1: N칰mero de art칤culos publicados y modificados por a침o
        st.subheader("N칰mero de art칤culos publicados y modificados por a침o")
        col1, col2, col3 = st.columns(3)

        # Gr치fico de publicaciones por a침o
        with col1:
            st.write("N칰mero de art칤culos publicados por a침o")
            df_filtered['A침o de Publicaci칩n'] = df_filtered['Fecha de Publicaci칩n'].dt.year
            articles_per_year = df_filtered.groupby('A침o de Publicaci칩n').size().reset_index(name='N칰mero de Art칤culos')
            
            chart1 = alt.Chart(articles_per_year).mark_bar().encode(
                x='A침o de Publicaci칩n:O',
                y='N칰mero de Art칤culos:Q',
            ).properties(width=250)  # Ajuste de ancho para caber en la columna

            st.altair_chart(chart1)

        # Gr치fico de modificaciones por a침o
        with col2:
            st.write("N칰mero de art칤culos modificados por a침o")
            df_filtered['A침o de Modificaci칩n'] = df_filtered['Fecha de Modificaci칩n'].dt.year
            modifications_per_year = df_filtered.groupby('A침o de Modificaci칩n').size().reset_index(name='N칰mero de Art칤culos Modificados')

            chart_modifications = alt.Chart(modifications_per_year).mark_bar(color='orange').encode(
                x='A침o de Modificaci칩n:O',
                y='N칰mero de Art칤culos Modificados:Q',
            ).properties(width=250)  # Ajuste de ancho para caber en la columna

            st.altair_chart(chart_modifications)

        # Gr치fico de la evoluci칩n acumulada de art칤culos publicados a lo largo del tiempo
        with col3:
            st.write("Evoluci칩n acumulada de art칤culos publicados")
            
            # Ordenar por fecha de publicaci칩n
            df_filtered = df_filtered.sort_values(by='Fecha de Publicaci칩n')  
            
            # Generar la columna de acumulado
            df_filtered['Art칤culos Acumulados'] = range(1, len(df_filtered) + 1)

            articles_time_series = df_filtered[['Fecha de Publicaci칩n', 'Art칤culos Acumulados']]

            # Crear el gr치fico de l칤nea
            chart_cumulative = alt.Chart(articles_time_series).mark_line(color='green').encode(
                x='Fecha de Publicaci칩n:T',
                y='Art칤culos Acumulados:Q',
            ).properties(width=250)  # Ajuste de ancho para caber en la columna

            st.altair_chart(chart_cumulative)




    else:
        st.write("Por favor, ingresa al menos una URL.")
